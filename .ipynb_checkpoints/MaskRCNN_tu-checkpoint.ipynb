{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from skimage import transform\n",
    "import cv2\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# EXTENSIONS_SCAN = ['.bin']\n",
    "EXTENSIONS_LABEL = ['.npy']\n",
    "EXTENSIONS_IMAGE = ['.png']\n",
    "\n",
    "def is_label(filename):\n",
    "    return any(filename.endswith(ext) for ext in EXTENSIONS_LABEL)\n",
    "\n",
    "\n",
    "def is_image(filename):\n",
    "    return any(filename.endswith(ext) for ext in EXTENSIONS_IMAGE)\n",
    "\n",
    "\n",
    "\n",
    "class PennFudanDataset(object):\n",
    "    def __init__(self, transforms):\n",
    "#         self.root = root\n",
    "        self.transforms = transforms\n",
    "        # 모든 이미지 파일들을 읽고, 정렬하여\n",
    "        # 이미지와 분할 마스크 정렬을 확인합니다\n",
    "        \n",
    "        # placeholder for filenames\n",
    "        self.imgs = []\n",
    "        self.masks = []\n",
    "\n",
    "        # get paths for each\n",
    "        # scan_path = os.path.join(\"/mnt/han/lidar-bonnetal/train/tasks/semantic/cdataset/dataset/sequences\", seq,\n",
    "        #                          \"velodyne\")\n",
    "        label_path = os.path.join(\"/home/kkm/Desktop/PennFudanPed/elipse_test\")\n",
    "        image_path = os.path.join(\"/home/kkm/Desktop/PennFudanPed/elipse_test\")\n",
    "\n",
    "\n",
    "\n",
    "        # get files\n",
    "        # scan_files = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "        #     os.path.expanduser(scan_path)) for f in fn if is_scan(f)]\n",
    "        label_files = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "          os.path.expanduser(label_path)) for f in fn if is_label(f)]\n",
    "        image_files = [os.path.join(dp, f) for dp, dn, fn in os.walk(\n",
    "          os.path.expanduser(image_path)) for f in fn if is_image(f)]\n",
    "\n",
    "        # scan_files.sort()\n",
    "        label_files.sort()\n",
    "        image_files.sort()\n",
    "        self.imgs.extend(image_files)\n",
    "        self.masks.extend(label_files)\n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 이미지와 마스크를 읽어옵니다\n",
    "        img_path =  self.imgs[idx]\n",
    "        mask_path = self.masks[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        # 분할 마스크는 RGB로 변환하지 않음을 유의하세요\n",
    "        # 왜냐하면 각 색상은 다른 인스턴스에 해당하며, 0은 배경에 해당합니다\n",
    "        mask = np.load(mask_path, allow_pickle=True)\n",
    "        # numpy 배열을 PIL 이미지로 변환합니다\n",
    "#         mask = np.array(mask)\n",
    "        # 인스턴스들은 다른 색들로 인코딩 되어 있습니다.\n",
    "        obj_ids = np.unique(mask)\n",
    "#         # 첫번째 id 는 배경이라 제거합니다\n",
    "        obj_ids = obj_ids[1:]\n",
    "        # 컬러 인코딩된 마스크를 바이너리 마스크 세트로 나눕니다\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "\n",
    "        # 각 마스크의 바운딩 박스 좌표를 얻습니다\n",
    "        num_objs = len(obj_ids)\n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        # 모든 것을 torch.Tensor 타입으로 변환합니다\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # 객체 종류는 한 종류만 존재합니다(역자주: 예제에서는 사람만이 대상입니다)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        # 모든 인스턴스는 군중(crowd) 상태가 아님을 가정합니다\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torchvision.models.detection import roi_heads\n",
    "roi_heads??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torchvision.ops import boxes as box_ops\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.ops import roi_align\n",
    "from scipy import ndimage\n",
    "from torchvision.models.detection import roi_heads\n",
    "\n",
    "def getweightmap(mask):\n",
    "#     assert mask.size > 0\n",
    "    w_c = np.empty(mask.shape)\n",
    "    frac0 = torch.mean((mask == 0).float())      # background\n",
    "    frac1 = torch.mean((mask == 1).float())        # instance\n",
    "    #     assert frac0 > 0\n",
    "    #     assert frac1 > 0\n",
    "    # Calculate weight map\n",
    "    w_c[mask == 0] = 0.5 / (frac0)\n",
    "    w_c[mask == 1] = 0.5 / (frac1)\n",
    "    return w_c\n",
    "\n",
    "def getunetweightmap(masks, w0=10, sigma=500):\n",
    "    weight_f = []\n",
    "    for i in np.arange(len(masks)):\n",
    "        masks = masks[i]\n",
    "        merged_mask = torch.sum(masks,dim=0)\n",
    "        weight = np.zeros(merged_mask.shape)\n",
    "        # calculate weight for important pixels\n",
    "        distances = np.array([ndimage.distance_transform_edt(m == 0) for m in masks])\n",
    "        shortest_dist = np.sort(distances, axis=0)     \n",
    "        # distance to the border of the nearest cell   \n",
    "        d1 = shortest_dist[0]\n",
    "        # distance to the border of the second nearest cell\n",
    "        d2 = shortest_dist[1] if len(shortest_dist) > 1 else np.zeros(d1.shape)\n",
    "        w_b = np.exp(-((d1 + d2) ** 2 / (2 * (sigma ** 2))))\n",
    "        w_c = getweightmap(merged_mask)\n",
    "        w = w_c +  (w0 * w_b)\n",
    "        # min = np.min(w[np.where(w>0)])\n",
    "        # weight = (merged_mask == 1) * (w-min)\n",
    "        weight = (masks == 1).float().mul(torch.from_numpy(w).float()).int()\n",
    "#         weight = w\n",
    "#         weight2=weight.unsqueeze(0)\n",
    "#         weight3=torch.cat((weight2,weight2),dim=0)\n",
    "#         weight4=torch.cat((weight3,weight2),dim=0)\n",
    "#         weight5=torch.cat((weight4,weight2),dim=0)\n",
    "        weight_f.append(weight)\n",
    "\n",
    "    return weight_f\n",
    "    \n",
    "def maskrcnn_loss(mask_logits, proposals, gt_masks, gt_labels, mask_matched_idxs):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        proposals (list[BoxList])\n",
    "        mask_logits (Tensor)\n",
    "        targets (list[BoxList])\n",
    "\n",
    "    Return:\n",
    "        mask_loss (Tensor): scalar tensor containing the loss\n",
    "    \"\"\"\n",
    "#     gt_masks2 = gt_masks.cpu()\n",
    "#     W_mask_tmp = (np.array(gt_masks) == 1 ).astype(np.float)\n",
    "#     W_merged_mask = np.max(gt_masks, axis=2)       \n",
    "    W_masks = getunetweightmap(gt_masks, w0=10, sigma=100)\n",
    "\n",
    "    discretization_size = mask_logits.shape[-1]\n",
    "    labels = [l[idxs] for l, idxs in zip(gt_labels, mask_matched_idxs)]\n",
    "    \n",
    "    mask_targets = [\n",
    "        project_masks_on_boxes(m, p, i, discretization_size)\n",
    "        for m, p, i in zip(gt_masks, proposals, mask_matched_idxs)\n",
    "    ]\n",
    "    \n",
    "    W_targets = [\n",
    "        project_masks_on_boxes(m, p, i, discretization_size)\n",
    "        for m, p, i in zip(W_masks, proposals, mask_matched_idxs)\n",
    "    ]\n",
    "\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    mask_targets = torch.cat(mask_targets, dim=0)\n",
    "    \n",
    "    W_targets = torch.cat(W_targets, dim=0)\n",
    "    \n",
    "\n",
    "    # torch.mean (in binary_cross_entropy_with_logits) doesn't\n",
    "    # accept empty tensors, so handle it separately\n",
    "    if mask_targets.numel() == 0:\n",
    "        return mask_logits.sum() * 0\n",
    "\n",
    "    mask_loss = F.binary_cross_entropy_with_logits(\n",
    "        mask_logits[torch.arange(labels.shape[0], device=labels.device), labels], mask_targets\n",
    "#         ,pos_weight=W_targets\n",
    "    )\n",
    "    return mask_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def project_masks_on_boxes(gt_masks, boxes, matched_idxs, M):\n",
    "    \"\"\"\n",
    "    Given segmentation masks and the bounding boxes corresponding\n",
    "    to the location of the masks in the image, this function\n",
    "    crops and resizes the masks in the position defined by the\n",
    "    boxes. This prepares the masks for them to be fed to the\n",
    "    loss computation as the targets.\n",
    "    \"\"\"\n",
    "    matched_idxs = matched_idxs.to(boxes)\n",
    "    rois = torch.cat([matched_idxs[:, None], boxes], dim=1)\n",
    "    gt_masks = gt_masks[:, None].to(rois)\n",
    "    return roi_align(gt_masks, rois, (M, M), 1)[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torchvision.models.detection import roi_heads\n",
    "roi_heads.maskrcnn_loss = maskrcnn_loss\n",
    "roi_heads.project_masks_on_boxes = project_masks_on_boxes\n",
    "\n",
    "roi_heads??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "roi_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # COCO 에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # 분류를 위한 입력 특징 차원을 얻습니다\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # 미리 학습된 헤더를 새로운 것으로 바꿉니다\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # 마스크 분류기를 위한 입력 특징들의 차원을 얻습니다\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # 마스크 예측기를 새로운 것으로 바꿉니다\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "#     model.roi_heads.maskrcnn_loss = maskrcnn_loss_W\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # (역자주: 학습시 50% 확률로 학습 영상을 좌우 반전 변환합니다)\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dimension out of range (expected to be in range of [-3, 2], but got 3) (maybe_wrap_dim at /opt/conda/conda-bld/pytorch_1565272279342/work/c10/core/WrapDimMinimal.h:20)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7fa5e9732e37 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x17dbd5c (0x7fa5ed086d5c in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/lib/libtorch.so)\nframe #2: at::native::size(at::Tensor const&, long) + 0x20 (0x7fa5ed087010 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/lib/libtorch.so)\nframe #3: ROIAlign_forward_cpu(at::Tensor const&, at::Tensor const&, float, int, int, int) + 0x460 (0x7fa5a1addd8c in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #4: ROIAlign_forward(at::Tensor const&, at::Tensor const&, float, int, int, int) + 0x92 (0x7fa5a1ab56a6 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #5: <unknown function> + 0x5e31e (0x7fa5a1ad731e in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #6: <unknown function> + 0x5b2d8 (0x7fa5a1ad42d8 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #7: <unknown function> + 0x56deb (0x7fa5a1acfdeb in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #8: <unknown function> + 0x57090 (0x7fa5a1ad0090 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #9: <unknown function> + 0x48047 (0x7fa5a1ac1047 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #10: _PyCFunction_FastCallDict + 0x154 (0x5622e79b9a14 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #11: <unknown function> + 0x19aa5c (0x5622e7a41a5c in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #13: PyEval_EvalCodeEx + 0x329 (0x5622e7a3c969 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #14: <unknown function> + 0x196784 (0x5622e7a3d784 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #15: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #16: THPFunction_apply(_object*, _object*) + 0x98f (0x7fa61adb44bf in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #17: _PyCFunction_FastCallDict + 0x91 (0x5622e79b9951 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #18: <unknown function> + 0x19aa5c (0x5622e7a41a5c in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #19: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #20: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #21: <unknown function> + 0x194e51 (0x5622e7a3be51 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #22: <unknown function> + 0x19ab35 (0x5622e7a41b35 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #23: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #24: <unknown function> + 0x194c1b (0x5622e7a3bc1b in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #25: <unknown function> + 0x19ab35 (0x5622e7a41b35 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #26: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #27: <unknown function> + 0x194166 (0x5622e7a3b166 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #28: <unknown function> + 0x194e51 (0x5622e7a3be51 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #29: <unknown function> + 0x19ab35 (0x5622e7a41b35 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #30: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #31: <unknown function> + 0x1942ce (0x5622e7a3b2ce in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #32: <unknown function> + 0x194e51 (0x5622e7a3be51 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #33: <unknown function> + 0x19ab35 (0x5622e7a41b35 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #34: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #35: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #36: _PyFunction_FastCallDict + 0x3da (0x5622e7a3c54a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #37: _PyObject_FastCallDict + 0x26f (0x5622e79b9ddf in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #38: _PyObject_Call_Prepend + 0x63 (0x5622e79be873 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #39: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #40: _PyEval_EvalFrameDefault + 0x196b (0x5622e7a658bb in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #41: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #42: _PyFunction_FastCallDict + 0x1bc (0x5622e7a3c32c in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #43: _PyObject_FastCallDict + 0x26f (0x5622e79b9ddf in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #44: _PyObject_Call_Prepend + 0x63 (0x5622e79be873 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #45: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #46: <unknown function> + 0x16c211 (0x5622e7a13211 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #47: _PyObject_FastCallDict + 0x8b (0x5622e79b9bfb in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #48: <unknown function> + 0x19abae (0x5622e7a41bae in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #49: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #50: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #51: _PyFunction_FastCallDict + 0x3da (0x5622e7a3c54a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #52: _PyObject_FastCallDict + 0x26f (0x5622e79b9ddf in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #53: _PyObject_Call_Prepend + 0x63 (0x5622e79be873 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #54: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x196b (0x5622e7a658bb in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #56: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #57: _PyFunction_FastCallDict + 0x1bc (0x5622e7a3c32c in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #58: _PyObject_FastCallDict + 0x26f (0x5622e79b9ddf in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #59: _PyObject_Call_Prepend + 0x63 (0x5622e79be873 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #60: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #61: <unknown function> + 0x16c211 (0x5622e7a13211 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #62: _PyObject_FastCallDict + 0x8b (0x5622e79b9bfb in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #63: <unknown function> + 0x19abae (0x5622e7a41bae in /home/kkm/anaconda3/envs/maskr/bin/python)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e9be1ea49e56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# 1 에포크동안 학습하고, 10회 마다 출력합니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m     \u001b[0;31m# 학습률을 업데이트 합니다\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/models/detection/generalized_rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposal_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdetector_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroi_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_image_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 loss_mask = maskrcnn_loss(\n\u001b[1;32m    670\u001b[0m                     \u001b[0mmask_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_proposals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m                     gt_masks, gt_labels, pos_matched_idxs)\n\u001b[0m\u001b[1;32m    672\u001b[0m                 \u001b[0mloss_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mmaskrcnn_loss\u001b[0;34m(mask_logits, proposals, gt_masks, gt_labels, mask_matched_idxs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     W_targets = [\n\u001b[1;32m    193\u001b[0m         \u001b[0mproject_masks_on_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscretization_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_matched_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     ]\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    192\u001b[0m     W_targets = [\n\u001b[1;32m    193\u001b[0m         \u001b[0mproject_masks_on_boxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscretization_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproposals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_matched_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     ]\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/models/detection/roi_heads.py\u001b[0m in \u001b[0;36mproject_masks_on_boxes\u001b[0;34m(gt_masks, boxes, matched_idxs, M)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mrois\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmatched_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0mgt_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt_masks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mroi_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/ops/roi_align.py\u001b[0m in \u001b[0;36mroi_align\u001b[0;34m(input, boxes, output_size, spatial_scale, sampling_ratio)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mrois\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_boxes_to_roi_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrois\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_RoIAlignFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrois\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampling_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/ops/roi_align.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, input, roi, output_size, spatial_scale, sampling_ratio)\u001b[0m\n\u001b[1;32m     22\u001b[0m         output = _C.roi_align_forward(\n\u001b[1;32m     23\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial_scale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             output_size[0], output_size[1], sampling_ratio)\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Dimension out of range (expected to be in range of [-3, 2], but got 3) (maybe_wrap_dim at /opt/conda/conda-bld/pytorch_1565272279342/work/c10/core/WrapDimMinimal.h:20)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x47 (0x7fa5e9732e37 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x17dbd5c (0x7fa5ed086d5c in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/lib/libtorch.so)\nframe #2: at::native::size(at::Tensor const&, long) + 0x20 (0x7fa5ed087010 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/lib/libtorch.so)\nframe #3: ROIAlign_forward_cpu(at::Tensor const&, at::Tensor const&, float, int, int, int) + 0x460 (0x7fa5a1addd8c in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #4: ROIAlign_forward(at::Tensor const&, at::Tensor const&, float, int, int, int) + 0x92 (0x7fa5a1ab56a6 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #5: <unknown function> + 0x5e31e (0x7fa5a1ad731e in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #6: <unknown function> + 0x5b2d8 (0x7fa5a1ad42d8 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #7: <unknown function> + 0x56deb (0x7fa5a1acfdeb in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #8: <unknown function> + 0x57090 (0x7fa5a1ad0090 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #9: <unknown function> + 0x48047 (0x7fa5a1ac1047 in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torchvision/_C.cpython-36m-x86_64-linux-gnu.so)\nframe #10: _PyCFunction_FastCallDict + 0x154 (0x5622e79b9a14 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #11: <unknown function> + 0x19aa5c (0x5622e7a41a5c in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #12: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #13: PyEval_EvalCodeEx + 0x329 (0x5622e7a3c969 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #14: <unknown function> + 0x196784 (0x5622e7a3d784 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #15: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #16: THPFunction_apply(_object*, _object*) + 0x98f (0x7fa61adb44bf in /home/kkm/anaconda3/envs/maskr/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #17: _PyCFunction_FastCallDict + 0x91 (0x5622e79b9951 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #18: <unknown function> + 0x19aa5c (0x5622e7a41a5c in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #19: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #20: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #21: <unknown function> + 0x194e51 (0x5622e7a3be51 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #22: <unknown function> + 0x19ab35 (0x5622e7a41b35 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #23: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #24: <unknown function> + 0x194c1b (0x5622e7a3bc1b in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #25: <unknown function> + 0x19ab35 (0x5622e7a41b35 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #26: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #27: <unknown function> + 0x194166 (0x5622e7a3b166 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #28: <unknown function> + 0x194e51 (0x5622e7a3be51 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #29: <unknown function> + 0x19ab35 (0x5622e7a41b35 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #30: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #31: <unknown function> + 0x1942ce (0x5622e7a3b2ce in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #32: <unknown function> + 0x194e51 (0x5622e7a3be51 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #33: <unknown function> + 0x19ab35 (0x5622e7a41b35 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #34: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #35: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #36: _PyFunction_FastCallDict + 0x3da (0x5622e7a3c54a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #37: _PyObject_FastCallDict + 0x26f (0x5622e79b9ddf in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #38: _PyObject_Call_Prepend + 0x63 (0x5622e79be873 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #39: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #40: _PyEval_EvalFrameDefault + 0x196b (0x5622e7a658bb in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #41: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #42: _PyFunction_FastCallDict + 0x1bc (0x5622e7a3c32c in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #43: _PyObject_FastCallDict + 0x26f (0x5622e79b9ddf in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #44: _PyObject_Call_Prepend + 0x63 (0x5622e79be873 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #45: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #46: <unknown function> + 0x16c211 (0x5622e7a13211 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #47: _PyObject_FastCallDict + 0x8b (0x5622e79b9bfb in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #48: <unknown function> + 0x19abae (0x5622e7a41bae in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #49: _PyEval_EvalFrameDefault + 0x30a (0x5622e7a6425a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #50: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #51: _PyFunction_FastCallDict + 0x3da (0x5622e7a3c54a in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #52: _PyObject_FastCallDict + 0x26f (0x5622e79b9ddf in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #53: _PyObject_Call_Prepend + 0x63 (0x5622e79be873 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #54: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #55: _PyEval_EvalFrameDefault + 0x196b (0x5622e7a658bb in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #56: <unknown function> + 0x193fd4 (0x5622e7a3afd4 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #57: _PyFunction_FastCallDict + 0x1bc (0x5622e7a3c32c in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #58: _PyObject_FastCallDict + 0x26f (0x5622e79b9ddf in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #59: _PyObject_Call_Prepend + 0x63 (0x5622e79be873 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #60: PyObject_Call + 0x3e (0x5622e79b981e in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #61: <unknown function> + 0x16c211 (0x5622e7a13211 in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #62: _PyObject_FastCallDict + 0x8b (0x5622e79b9bfb in /home/kkm/anaconda3/envs/maskr/bin/python)\nframe #63: <unknown function> + 0x19abae (0x5622e7a41bae in /home/kkm/anaconda3/envs/maskr/bin/python)\n"
     ]
    }
   ],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "# def main():\n",
    "# 학습을 GPU로 진행하되 GPU가 가용하지 않으면 CPU로 합니다\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# 우리 데이터셋은 두 개의 클래스만 가집니다 - 배경과 사람\n",
    "num_classes = 2\n",
    "# 데이터셋과 정의된 변환들을 사용합니다\n",
    "dataset = PennFudanDataset( get_transform(train=True))\n",
    "dataset_test = PennFudanDataset(get_transform(train=False))\n",
    "\n",
    "# dataset = PennFudanDataset()\n",
    "# dataset_test = PennFudanDataset()\n",
    "\n",
    "\n",
    "# 데이터셋을 학습용과 테스트용으로 나눕니다(역자주: 여기서는 전체의 50개를 테스트에, 나머지를 학습에 사용합니다)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "# dataset = torch.utils.data.Subset(dataset, indices[:4])\n",
    "# dataset_test = torch.utils.data.Subset(dataset_test, indices[:4])\n",
    "\n",
    "# 데이터 로더를 학습용과 검증용으로 정의합니다\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# 도움 함수를 이용해 모델을 가져옵니다\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# 모델을 GPU나 CPU로 옮깁니다\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "# 옵티마이저(Optimizer)를 만듭니다\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# 학습률 스케쥴러를 만듭니다\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "# 10 에포크만큼 학습해봅시다\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1 에포크동안 학습하고, 10회 마다 출력합니다\n",
    "    train_one_epoch(model, optimizer, data_loader, torch.device('cpu'), epoch, print_freq=10)\n",
    "    # 학습률을 업데이트 합니다\n",
    "    lr_scheduler.step()\n",
    "    # 테스트 데이터셋에서 평가를 합니다\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f0fd00fbef0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,_,_ =iter(data_loader)\n",
    "targets = a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gt_masks = [t[\"masks\"] for t in targets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gt_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_masks[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_f = []\n",
    "w0=10\n",
    "sigma=100\n",
    "for i in np.arange(len(gt_masks)):\n",
    "    masks = gt_masks[i]\n",
    "    merged_mask = torch.sum(masks,dim=0)\n",
    "    weight = np.zeros(merged_mask.shape)\n",
    "    # calculate weight for important pixels\n",
    "    distances = np.array([ndimage.distance_transform_edt(m == 0) for m in masks])\n",
    "    shortest_dist = np.sort(distances, axis=0)     \n",
    "    # distance to the border of the nearest cell   \n",
    "    d1 = shortest_dist[0]\n",
    "    # distance to the border of the second nearest cell\n",
    "    d2 = shortest_dist[1] if len(shortest_dist) > 1 else np.zeros(d1.shape)\n",
    "    w_b = np.exp(-((d1 + d2) ** 2 / (2 * (sigma ** 2))))\n",
    "    w_c = getweightmap(merged_mask)\n",
    "    w = w_c +  (w0 * w_b)\n",
    "    # min = np.min(w[np.where(w>0)])\n",
    "    # weight = (merged_mask == 1) * (w-min)\n",
    "    weight = (merged_mask == 1).float().mul(torch.from_numpy(w).float()).int()\n",
    "    # weight = w\n",
    "    # return weight\n",
    "    weight2=weight.unsqueeze(0)\n",
    "    weight3=torch.cat((weight2,weight2),dim=0)\n",
    "    weight4=torch.cat((weight3,weight2),dim=0)\n",
    "    weight5=torch.cat((weight4,weight2),dim=0)\n",
    "    weight_f.append(weight5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_f[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_f[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_masks[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_f[0].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_f[0].int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def maskrcnn_loss(mask_logits, proposals, gt_masks, gt_labels, mask_matched_idxs):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        proposals (list[BoxList])\n",
    "        mask_logits (Tensor)\n",
    "        targets (list[BoxList])\n",
    "\n",
    "    Return:\n",
    "        mask_loss (Tensor): scalar tensor containing the loss\n",
    "    \"\"\"\n",
    "#     gt_masks2 = gt_masks.cpu()\n",
    "#     W_mask_tmp = (np.array(gt_masks) == 1 ).astype(np.float)\n",
    "#     W_merged_mask = np.max(gt_masks, axis=2)       \n",
    "    W_masks = getunetweightmap(gt_masks, w0=10, sigma=5)\n",
    "\n",
    "    discretization_size = mask_logits.shape[-1]\n",
    "    labels = [l[idxs] for l, idxs in zip(gt_labels, mask_matched_idxs)]\n",
    "    \n",
    "    mask_targets = [\n",
    "        project_masks_on_boxes(m, p, i, discretization_size)\n",
    "        for m, p, i in zip(gt_masks, proposals, mask_matched_idxs)\n",
    "    ]\n",
    "    \n",
    "    W_targets = [\n",
    "    project_masks_on_boxes(m, p, i, discretization_size)\n",
    "    for m, p, i in zip(W_masks, proposals, mask_matched_idxs)\n",
    "    ]\n",
    "\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    mask_targets = torch.cat(mask_targets, dim=0)\n",
    "    \n",
    "    W_targets = torch.cat(W_targets, dim=0)\n",
    "    \n",
    "\n",
    "    # torch.mean (in binary_cross_entropy_with_logits) doesn't\n",
    "    # accept empty tensors, so handle it separately\n",
    "    if mask_targets.numel() == 0:\n",
    "        return mask_logits.sum() * 0\n",
    "\n",
    "    mask_loss = F.binary_cross_entropy_with_logits(\n",
    "        mask_logits[torch.arange(labels.shape[0], device=labels.device), labels], mask_targets\n",
    "        ,pos_weight=W_targets\n",
    "    )\n",
    "    return mask_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roi_heads??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getweightmap(mask):\n",
    "#     assert mask.size > 0\n",
    "    mask = mask[0]\n",
    "    w_c = torch.empty(mask.shape)\n",
    "    frac0 = torch.sum(mask == 0) / float(mask.size)        # background\n",
    "    frac1 = torch.sum(mask == 1) / float(mask.size)        # instance\n",
    "#     assert frac0 > 0\n",
    "#     assert frac1 > 0\n",
    "    # Calculate weight map\n",
    "    w_c[mask == 0] = 0.5 / (frac0)\n",
    "    w_c[mask == 1] = 0.5 / (frac1)\n",
    "    return w_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gt_masks[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]],\n",
       "\n",
       "        [[True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         ...,\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True],\n",
       "         [True, True, True,  ..., True, True, True]]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mask == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = gt_masks[0]\n",
    "w_c = torch.empty(mask.shape)\n",
    "frac0 = torch.mean((mask == 0).float())      # background\n",
    "frac1 = torch.mean((mask == 1).float())      # instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_c[mask == 0] = 0.5 / (frac0)\n",
    "w_c[mask == 1] = 0.5 / (frac1)\n",
    "w_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0=10\n",
    "sigma=5\n",
    "masks = gt_masks[0]\n",
    "weight = torch.zeros(masks.shape)\n",
    "# calculate weight for important pixels\n",
    "distances = np.array([ndimage.distance_transform_edt(m == 0) for m in masks])\n",
    "shortest_dist = np.sort(distances, axis=0)     \n",
    "# distance to the border of the nearest cell   \n",
    "d1 = shortest_dist[0]\n",
    "# distance to the border of the second nearest cell\n",
    "d2 = shortest_dist[1] if len(shortest_dist) > 1 else np.zeros(d1.shape)\n",
    "w_b = np.exp(-((d1 + d2) ** 2 / (2 * (sigma ** 2)))).astype(np.float32)\n",
    "# w_c = getweightmap(masks)\n",
    "w = w_c +  torch.from_numpy(w_b).mul(w0)\n",
    "# min = np.min(w[np.where(w>0)])\n",
    "# weight = (merged_mask == 1) * (w-min)\n",
    "weight = (masks == 1).float() * w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(w_b).mul(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 1.4012985e-45, 2.8025969e-45, ..., 1.6275797e-26,\n",
       "       1.6277223e-26, 1.6277724e-26], dtype=float32)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(w_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.50693464, 36.551033  ], dtype=float32)"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.      , 36.551033], dtype=float32)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(w_b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(weight[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getweightmap(mask):\n",
    "#     assert mask.size > 0\n",
    "    w_c = np.empty(mask.shape)\n",
    "    frac0 = torch.mean((mask == 0).float())      # background\n",
    "    frac1 = torch.mean((mask == 1).float())        # instance\n",
    "    #     assert frac0 > 0\n",
    "    #     assert frac1 > 0\n",
    "    # Calculate weight map\n",
    "    w_c[mask == 0] = 0.5 / (frac0)\n",
    "    w_c[mask == 1] = 0.5 / (frac1)\n",
    "    return w_c\n",
    "\n",
    "def getunetweightmap(masks, w0=10, sigma=100):\n",
    "    weight_f = []\n",
    "    for i in np.arange(len(gt_masks)):\n",
    "        masks = gt_masks[i]\n",
    "        merged_mask = torch.sum(masks,dim=0)\n",
    "        weight = np.zeros(merged_mask.shape)\n",
    "        # calculate weight for important pixels\n",
    "        distances = np.array([ndimage.distance_transform_edt(m == 0) for m in masks])\n",
    "        shortest_dist = np.sort(distances, axis=0)     \n",
    "        # distance to the border of the nearest cell   \n",
    "        d1 = shortest_dist[0]\n",
    "        # distance to the border of the second nearest cell\n",
    "        d2 = shortest_dist[1] if len(shortest_dist) > 1 else np.zeros(d1.shape)\n",
    "        w_b = np.exp(-((d1 + d2) ** 2 / (2 * (sigma ** 2))))\n",
    "        w_c = getweightmap(merged_mask)\n",
    "        w = w_c +  (w0 * w_b)\n",
    "        # min = np.min(w[np.where(w>0)])\n",
    "        # weight = (merged_mask == 1) * (w-min)\n",
    "        weight = (merged_mask == 1).float().mul(torch.from_numpy(w).float())\n",
    "        # weight = w\n",
    "       \n",
    "        weight_f.append(weight)\n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mask = torch.sum(masks,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "masks = gt_masks[0]\n",
    "mask = torch.sum(masks,dim=0)\n",
    "w_c = np.empty(mask.shape)\n",
    "frac0 = torch.mean((mask == 0).float())      # background\n",
    "frac1 = torch.mean((mask == 1).float())        # instance\n",
    "#     assert frac0 > 0\n",
    "#     assert frac1 > 0\n",
    "# Calculate weight map\n",
    "w_c[mask == 0] = 0.5 / (frac0)\n",
    "w_c[mask == 1] = 0.5 / (frac1)\n",
    "# return w_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_f = []\n",
    "for i in np.arange(len(gt_masks)):\n",
    "    masks = gt_masks[i]\n",
    "    merged_mask = torch.sum(masks,dim=0)\n",
    "    weight = np.zeros(merged_mask.shape)\n",
    "    # calculate weight for important pixels\n",
    "    distances = np.array([ndimage.distance_transform_edt(m == 0) for m in masks])\n",
    "    shortest_dist = np.sort(distances, axis=0)     \n",
    "    # distance to the border of the nearest cell   \n",
    "    d1 = shortest_dist[0]\n",
    "    # distance to the border of the second nearest cell\n",
    "    d2 = shortest_dist[1] if len(shortest_dist) > 1 else np.zeros(d1.shape)\n",
    "    w_b = np.exp(-((d1 + d2) ** 2 / (2 * (sigma ** 2))))\n",
    "    w_c = getweightmap(merged_mask)\n",
    "    w = w_c +  (w0 * w_b)\n",
    "    # min = np.min(w[np.where(w>0)])\n",
    "    # weight = (merged_mask == 1) * (w-min)\n",
    "    weight = (merged_mask == 1).float().mul(torch.from_numpy(w).float())\n",
    "    # weight = w\n",
    "    # return weight\n",
    "    weight2=weight.unsqueeze(0)\n",
    "    weight3=torch.cat((weight2,weight2),dim=0)\n",
    "    weight4=torch.cat((weight3,weight2),dim=0)\n",
    "    weight5=torch.cat((weight4,weight2),dim=0)\n",
    "    weight_f.append(weight5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weight_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 512])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_f[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in np.arange(len(gt_masks)):\n",
    "    masks = gt_masks[i]\n",
    "\n",
    "\n",
    "w0 = 10\n",
    "sigma = 100\n",
    "# def getunetweightmap(merged_mask, masks, w0=10, sigma=5):\n",
    "    # HxWxN to NxHxW\n",
    "masks = gt_masks[0]\n",
    "merged_mask = torch.sum(masks,dim=0)\n",
    "weight = np.zeros(merged_mask.shape)\n",
    "# calculate weight for important pixels\n",
    "distances = np.array([ndimage.distance_transform_edt(m == 0) for m in masks])\n",
    "shortest_dist = np.sort(distances, axis=0)     \n",
    "# distance to the border of the nearest cell   \n",
    "d1 = shortest_dist[0]\n",
    "# distance to the border of the second nearest cell\n",
    "d2 = shortest_dist[1] if len(shortest_dist) > 1 else np.zeros(d1.shape)\n",
    "w_b = np.exp(-((d1 + d2) ** 2 / (2 * (sigma ** 2))))\n",
    "w_c = getweightmap(merged_mask)\n",
    "w = w_c +  (w0 * w_b)\n",
    "# min = np.min(w[np.where(w>0)])\n",
    "# weight = (merged_mask == 1) * (w-min)\n",
    "weight = (merged_mask == 1).float().mul(torch.from_numpy(w).float())\n",
    "# weight = w\n",
    "# return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5065"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(weight))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(weight)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w_b = np.exp(-((d1 + d2) ** 2 / (2 * (sigma ** 2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(w0 * w_b).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.     , 12.07258], dtype=float32)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512, 512)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         ...,\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052]],\n",
       "\n",
       "        [[0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         ...,\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052]],\n",
       "\n",
       "        [[0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         ...,\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052]],\n",
       "\n",
       "        [[0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         ...,\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052],\n",
       "         [0.5052, 0.5052, 0.5052,  ..., 0.5052, 0.5052, 0.5052]]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "# COCO로 미리 학솝된 모델 읽기\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# 분류기를 새로운 것으로 교체하는데, num_classes는 사용자가 정의합니다\n",
    "num_classes = 2  # 1 클래스(사람) + 배경\n",
    "# 분류기에서 사용할 입력 특징의 차원 정보를 얻습니다\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# 미리 학습된 모델의 머리 부분을 새로운 것으로 교체합니다\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "\n",
    "# 분류 목적으로 미리 학습된 모델을 로드하고 특징들만을 리턴하도록 합니다\n",
    "backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n",
    "# Faster RCNN은 백본의 출력 채널 수를 알아야 합니다.\n",
    "# mobilenetV2의 경우 1280이므로 여기에 추가해야 합니다.\n",
    "backbone.out_channels = 1280\n",
    "\n",
    "# RPN(Region Proposal Network)이 5개의 서로 다른 크기와 3개의 다른 측면 비율(Aspect ratio)을 가진\n",
    "# 5 x 3개의 앵커를 공간 위치마다 생성하도록 합니다.\n",
    "# 각 특징 맵이 잠재적으로 다른 사이즈와 측면 비율을 가질 수 있기 때문에 Tuple[Tuple[int]] 타입을 가지도록 합니다.\n",
    "\n",
    "anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n",
    "                                   aspect_ratios=((0.5, 1.0, 2.0),))\n",
    "\n",
    "# 관심 영역의 자르기 및 재할당 후 자르기 크기를 수행하는 데 사용할 피쳐 맵을 정의합니다.\n",
    "# 만약 백본이 텐서를 리턴할때, featmap_names 는 [0] 이 될 것이라고 예상합니다.\n",
    "# 일반적으로 백본은 OrderedDict[Tensor] 타입을 리턴해야 합니다.\n",
    "# 그리고 특징맵에서 사용할 featmap_names 값을 정할 수 있습니다.\n",
    "roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],\n",
    "                                                output_size=7,\n",
    "                                                sampling_ratio=2)\n",
    "\n",
    "# 조각들을 Faster RCNN 모델로 합칩니다.\n",
    "model = FasterRCNN(backbone,\n",
    "                   num_classes=2,\n",
    "                   rpn_anchor_generator=anchor_generator,\n",
    "                   box_roi_pool=roi_pooler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "\n",
    "def get_model_instance_segmentation(num_classes):\n",
    "    # COCO 에서 미리 학습된 인스턴스 분할 모델을 읽어옵니다\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # 분류를 위한 입력 특징 차원을 얻습니다\n",
    "#     in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # 미리 학습된 헤더를 새로운 것으로 바꿉니다\n",
    "#     model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # 마스크 분류기를 위한 입력 특징들의 차원을 얻습니다\n",
    "#     in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "#     hidden_layer = 256\n",
    "    # 마스크 예측기를 새로운 것으로 바꿉니다\n",
    "#     model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "#                                                        hidden_layer,\n",
    "#                                                        num_classes)\n",
    "#     model.roi_heads.maskrcnn_loss = maskrcnn_loss_W\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.roi_heads??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = get_model_instance_segmentation(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torchvision.models.detection import mask_rcnn\n",
    "mask_rcnn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = get_model_instance_segmentation(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object `model.maskrcnn_loss` not found.\n"
     ]
    }
   ],
   "source": [
    "model.maskrcnn_loss??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from torchvision.models.detection import roi_heads\n",
    "roi_heads.maskrcnn_loss??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "F.binary_cross_entropy_with_logits??\n",
    "# torch.binary_cross_entropy_with_logits??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = get_model_instance_segmentation(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.roi_heads.maskrcnn_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "from torchvision.ops import boxes as box_ops\n",
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.ops import roi_align\n",
    "\n",
    "def getweightmap(mask):\n",
    "#     assert mask.size > 0\n",
    "    w_c = np.empty(mask.shape)\n",
    "    frac0 = np.sum(mask == 0) / float(mask.size)        # background\n",
    "    frac1 = np.sum(mask == 1) / float(mask.size)        # instance\n",
    "#     assert frac0 > 0\n",
    "#     assert frac1 > 0\n",
    "    # Calculate weight map\n",
    "    w_c[mask == 0] = 0.5 / (frac0)\n",
    "    w_c[mask == 1] = 0.5 / (frac1)\n",
    "    return w_c\n",
    "\n",
    "def getunetweightmap(merged_mask, masks, w0=10, sigma=5):\n",
    "    # HxWxN to NxHxW\n",
    "    masks = masks.transpose((2, 0, 1))\n",
    "    weight = np.zeros(merged_mask.shape)\n",
    "    # calculate weight for important pixels\n",
    "    distances = np.array([ndimage.distance_transform_edt(m == 0) for m in masks])\n",
    "    shortest_dist = np.sort(distances, axis=0)     \n",
    "    # distance to the border of the nearest cell   \n",
    "    d1 = shortest_dist[0]\n",
    "    # distance to the border of the second nearest cell\n",
    "    d2 = shortest_dist[1] if len(shortest_dist) > 1 else np.zeros(d1.shape)\n",
    "    w_b = np.exp(-((d1 + d2) ** 2 / (2 * (sigma ** 2)))).astype(np.float32)\n",
    "    w_c = getweightmap(merged_mask)\n",
    "    w = w_c +  w0 * w_b\n",
    "    # min = np.min(w[np.where(w>0)])\n",
    "    # weight = (merged_mask == 1) * (w-min)\n",
    "    weight = (merged_mask == 1) * w\n",
    "    # weight = w\n",
    "    return weight\n",
    "    \n",
    "def maskrcnn_loss(mask_logits, proposals, gt_masks, gt_labels, mask_matched_idxs):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        proposals (list[BoxList])\n",
    "        mask_logits (Tensor)\n",
    "        targets (list[BoxList])\n",
    "\n",
    "    Return:\n",
    "        mask_loss (Tensor): scalar tensor containing the loss\n",
    "    \"\"\"\n",
    "\n",
    "    W_mask_tmp = (gt_masks[0] > 0).astype(np.float)\n",
    "    W_merged_mask = np.max(W_mask_tmp, axis=2)       \n",
    "    W_masks = getunetweightmap(W_merged_mask, W_mask_tmp, w0=10, sigma=5)\n",
    "\n",
    "    discretization_size = mask_logits.shape[-1]\n",
    "    labels = [l[idxs] for l, idxs in zip(gt_labels, mask_matched_idxs)]\n",
    "    \n",
    "    mask_targets = [\n",
    "        project_masks_on_boxes(m, p, i, discretization_size)\n",
    "        for m, p, i in zip(gt_masks, proposals, mask_matched_idxs)\n",
    "    ]\n",
    "    \n",
    "    W_targets = [\n",
    "    project_masks_on_boxes(m, p, i, discretization_size)\n",
    "    for m, p, i in zip(W_masks, proposals, mask_matched_idxs)\n",
    "    ]\n",
    "\n",
    "    labels = torch.cat(labels, dim=0)\n",
    "    \n",
    "    mask_targets = torch.cat(mask_targets, dim=0)\n",
    "    \n",
    "    W_targets = torch.cat(W_targets, dim=0)\n",
    "    \n",
    "\n",
    "    # torch.mean (in binary_cross_entropy_with_logits) doesn't\n",
    "    # accept empty tensors, so handle it separately\n",
    "    if mask_targets.numel() == 0:\n",
    "        return mask_logits.sum() * 0\n",
    "\n",
    "    mask_loss = F.binary_cross_entropy_with_logits(\n",
    "        mask_logits[torch.arange(labels.shape[0], device=labels.device), labels], mask_targets\n",
    "        ,pos_weight=W_targets\n",
    "    )\n",
    "    return mask_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.roi_heads.maskrcnn_loss = maskrcnn_loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "dataset = PennFudanDataset(get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    " collate_fn=utils.collate_fn)\n",
    "# 학습 시\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# 추론 시\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)           # Returns predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output = model(images,targets) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "output??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "predictions = model(images) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # (역자주: 학습시 50% 확률로 학습 영상을 좌우 반전 변환합니다)\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from engine import train_one_epoch\n",
    "# train_one_epoch??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.roi_heads.maskrcnn_loss = maskrcnn_loss_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [0/3]  eta: 0:00:02  lr: 0.002502  loss: 0.0898 (0.0898)  loss_classifier: 0.0212 (0.0212)  loss_box_reg: 0.0110 (0.0110)  loss_mask: 0.0506 (0.0506)  loss_objectness: 0.0010 (0.0010)  loss_rpn_box_reg: 0.0061 (0.0061)  time: 0.7395  data: 0.2253  max mem: 2908\n",
      "Epoch: [0]  [2/3]  eta: 0:00:00  lr: 0.005000  loss: 0.0898 (0.0962)  loss_classifier: 0.0212 (0.0275)  loss_box_reg: 0.0129 (0.0125)  loss_mask: 0.0506 (0.0501)  loss_objectness: 0.0005 (0.0006)  loss_rpn_box_reg: 0.0054 (0.0055)  time: 0.5104  data: 0.0756  max mem: 3012\n",
      "Epoch: [0] Total time: 0:00:01 (0.5184 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:00  model_time: 0.0882 (0.0882)  evaluator_time: 0.0053 (0.0053)  time: 0.1867  data: 0.0905  max mem: 3012\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.0890 (0.0896)  evaluator_time: 0.0047 (0.0048)  time: 0.1156  data: 0.0197  max mem: 3012\n",
      "Test: Total time: 0:00:00 (0.1202 s / it)\n",
      "Averaged stats: model_time: 0.0890 (0.0896)  evaluator_time: 0.0047 (0.0048)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.837\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.904\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.904\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.828\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.950\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.894\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.950\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.877\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.904\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.904\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.866\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.195\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.922\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "Epoch: [1]  [0/3]  eta: 0:00:02  lr: 0.005000  loss: 0.0967 (0.0967)  loss_classifier: 0.0201 (0.0201)  loss_box_reg: 0.0175 (0.0175)  loss_mask: 0.0518 (0.0518)  loss_objectness: 0.0015 (0.0015)  loss_rpn_box_reg: 0.0058 (0.0058)  time: 0.7114  data: 0.2166  max mem: 3012\n",
      "Epoch: [1]  [2/3]  eta: 0:00:00  lr: 0.005000  loss: 0.0967 (0.0963)  loss_classifier: 0.0280 (0.0258)  loss_box_reg: 0.0135 (0.0144)  loss_mask: 0.0507 (0.0502)  loss_objectness: 0.0003 (0.0006)  loss_rpn_box_reg: 0.0058 (0.0053)  time: 0.5374  data: 0.0727  max mem: 3025\n",
      "Epoch: [1] Total time: 0:00:01 (0.5491 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:01  model_time: 0.1045 (0.1045)  evaluator_time: 0.0062 (0.0062)  time: 0.2025  data: 0.0893  max mem: 3025\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.1045 (0.1026)  evaluator_time: 0.0052 (0.0055)  time: 0.1302  data: 0.0199  max mem: 3025\n",
      "Test: Total time: 0:00:00 (0.1360 s / it)\n",
      "Averaged stats: model_time: 0.1045 (0.1026)  evaluator_time: 0.0052 (0.0055)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.838\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.901\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.901\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.826\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.190\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.905\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.905\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.894\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.901\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.901\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.885\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.945\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.945\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.939\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "Epoch: [2]  [0/3]  eta: 0:00:02  lr: 0.005000  loss: 0.0896 (0.0896)  loss_classifier: 0.0230 (0.0230)  loss_box_reg: 0.0091 (0.0091)  loss_mask: 0.0505 (0.0505)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0067 (0.0067)  time: 0.7632  data: 0.1960  max mem: 3025\n",
      "Epoch: [2]  [2/3]  eta: 0:00:00  lr: 0.005000  loss: 0.0953 (0.0964)  loss_classifier: 0.0230 (0.0255)  loss_box_reg: 0.0132 (0.0122)  loss_mask: 0.0505 (0.0500)  loss_objectness: 0.0002 (0.0020)  loss_rpn_box_reg: 0.0067 (0.0067)  time: 0.5193  data: 0.0670  max mem: 3025\n",
      "Epoch: [2] Total time: 0:00:01 (0.5275 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:00  model_time: 0.0885 (0.0885)  evaluator_time: 0.0054 (0.0054)  time: 0.1974  data: 0.1020  max mem: 3025\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.0902 (0.0909)  evaluator_time: 0.0053 (0.0051)  time: 0.1184  data: 0.0210  max mem: 3025\n",
      "Test: Total time: 0:00:00 (0.1230 s / it)\n",
      "Averaged stats: model_time: 0.0902 (0.0909)  evaluator_time: 0.0053 (0.0051)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.808\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.801\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.950\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.875\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.875\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.867\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.950\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.867\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.859\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.195\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.922\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3]  [0/3]  eta: 0:00:01  lr: 0.000500  loss: 0.0899 (0.0899)  loss_classifier: 0.0236 (0.0236)  loss_box_reg: 0.0104 (0.0104)  loss_mask: 0.0503 (0.0503)  loss_objectness: 0.0001 (0.0001)  loss_rpn_box_reg: 0.0055 (0.0055)  time: 0.6349  data: 0.1194  max mem: 3025\n",
      "Epoch: [3]  [2/3]  eta: 0:00:00  lr: 0.000500  loss: 0.0921 (0.0936)  loss_classifier: 0.0236 (0.0231)  loss_box_reg: 0.0127 (0.0140)  loss_mask: 0.0503 (0.0495)  loss_objectness: 0.0003 (0.0004)  loss_rpn_box_reg: 0.0060 (0.0065)  time: 0.4725  data: 0.0419  max mem: 3025\n",
      "Epoch: [3] Total time: 0:00:01 (0.4804 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:00  model_time: 0.0929 (0.0929)  evaluator_time: 0.0056 (0.0056)  time: 0.1912  data: 0.0904  max mem: 3025\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.0919 (0.0918)  evaluator_time: 0.0049 (0.0051)  time: 0.1181  data: 0.0196  max mem: 3025\n",
      "Test: Total time: 0:00:00 (0.1231 s / it)\n",
      "Averaged stats: model_time: 0.0919 (0.0918)  evaluator_time: 0.0049 (0.0051)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.832\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.820\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.889\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.868\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.860\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.195\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.922\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "Epoch: [4]  [0/3]  eta: 0:00:01  lr: 0.000500  loss: 0.0920 (0.0920)  loss_classifier: 0.0281 (0.0281)  loss_box_reg: 0.0093 (0.0093)  loss_mask: 0.0495 (0.0495)  loss_objectness: 0.0007 (0.0007)  loss_rpn_box_reg: 0.0043 (0.0043)  time: 0.6408  data: 0.1276  max mem: 3025\n",
      "Epoch: [4]  [2/3]  eta: 0:00:00  lr: 0.000500  loss: 0.0906 (0.0891)  loss_classifier: 0.0252 (0.0242)  loss_box_reg: 0.0093 (0.0107)  loss_mask: 0.0495 (0.0484)  loss_objectness: 0.0002 (0.0004)  loss_rpn_box_reg: 0.0047 (0.0055)  time: 0.4744  data: 0.0438  max mem: 3025\n",
      "Epoch: [4] Total time: 0:00:01 (0.4826 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:00  model_time: 0.0892 (0.0892)  evaluator_time: 0.0057 (0.0057)  time: 0.1900  data: 0.0930  max mem: 3025\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.0899 (0.0899)  evaluator_time: 0.0048 (0.0050)  time: 0.1164  data: 0.0200  max mem: 3025\n",
      "Test: Total time: 0:00:00 (0.1211 s / it)\n",
      "Averaged stats: model_time: 0.0899 (0.0899)  evaluator_time: 0.0048 (0.0050)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.845\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.833\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.910\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.910\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.900\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.879\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.871\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.940\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.940\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.933\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "Epoch: [5]  [0/3]  eta: 0:00:02  lr: 0.000500  loss: 0.0981 (0.0981)  loss_classifier: 0.0318 (0.0318)  loss_box_reg: 0.0147 (0.0147)  loss_mask: 0.0468 (0.0468)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0046 (0.0046)  time: 0.7411  data: 0.2229  max mem: 3025\n",
      "Epoch: [5]  [2/3]  eta: 0:00:00  lr: 0.000500  loss: 0.0901 (0.0913)  loss_classifier: 0.0263 (0.0251)  loss_box_reg: 0.0104 (0.0108)  loss_mask: 0.0494 (0.0499)  loss_objectness: 0.0002 (0.0002)  loss_rpn_box_reg: 0.0046 (0.0053)  time: 0.5120  data: 0.0748  max mem: 3025\n",
      "Epoch: [5] Total time: 0:00:01 (0.5216 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:01  model_time: 0.1043 (0.1043)  evaluator_time: 0.0058 (0.0058)  time: 0.2034  data: 0.0908  max mem: 3025\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.0921 (0.0938)  evaluator_time: 0.0050 (0.0051)  time: 0.1208  data: 0.0200  max mem: 3025\n",
      "Test: Total time: 0:00:00 (0.1279 s / it)\n",
      "Averaged stats: model_time: 0.0921 (0.0938)  evaluator_time: 0.0050 (0.0051)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.925\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.860\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.860\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.950\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.871\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.862\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.922\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6]  [0/3]  eta: 0:00:02  lr: 0.000050  loss: 0.0896 (0.0896)  loss_classifier: 0.0235 (0.0235)  loss_box_reg: 0.0126 (0.0126)  loss_mask: 0.0464 (0.0464)  loss_objectness: 0.0028 (0.0028)  loss_rpn_box_reg: 0.0042 (0.0042)  time: 0.7220  data: 0.1739  max mem: 3048\n",
      "Epoch: [6]  [2/3]  eta: 0:00:00  lr: 0.000050  loss: 0.0896 (0.0889)  loss_classifier: 0.0213 (0.0220)  loss_box_reg: 0.0094 (0.0102)  loss_mask: 0.0509 (0.0501)  loss_objectness: 0.0003 (0.0011)  loss_rpn_box_reg: 0.0059 (0.0055)  time: 0.5057  data: 0.0585  max mem: 3048\n",
      "Epoch: [6] Total time: 0:00:01 (0.5137 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:00  model_time: 0.0911 (0.0911)  evaluator_time: 0.0058 (0.0058)  time: 0.1960  data: 0.0970  max mem: 3048\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.0968 (0.0955)  evaluator_time: 0.0050 (0.0053)  time: 0.1233  data: 0.0210  max mem: 3048\n",
      "Test: Total time: 0:00:00 (0.1281 s / it)\n",
      "Averaged stats: model_time: 0.0968 (0.0955)  evaluator_time: 0.0050 (0.0053)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.798\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.790\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.925\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.860\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.860\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.850\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.950\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.871\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.862\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.922\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "Epoch: [7]  [0/3]  eta: 0:00:02  lr: 0.000050  loss: 0.0875 (0.0875)  loss_classifier: 0.0233 (0.0233)  loss_box_reg: 0.0102 (0.0102)  loss_mask: 0.0474 (0.0474)  loss_objectness: 0.0005 (0.0005)  loss_rpn_box_reg: 0.0060 (0.0060)  time: 0.6842  data: 0.1690  max mem: 3048\n",
      "Epoch: [7]  [2/3]  eta: 0:00:00  lr: 0.000050  loss: 0.0918 (0.0915)  loss_classifier: 0.0251 (0.0263)  loss_box_reg: 0.0098 (0.0090)  loss_mask: 0.0498 (0.0503)  loss_objectness: 0.0002 (0.0003)  loss_rpn_box_reg: 0.0058 (0.0056)  time: 0.5006  data: 0.0577  max mem: 3048\n",
      "Epoch: [7] Total time: 0:00:01 (0.5086 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:01  model_time: 0.0921 (0.0921)  evaluator_time: 0.0060 (0.0060)  time: 0.2041  data: 0.1044  max mem: 3048\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.0933 (0.1042)  evaluator_time: 0.0057 (0.0061)  time: 0.1342  data: 0.0222  max mem: 3048\n",
      "Test: Total time: 0:00:00 (0.1415 s / it)\n",
      "Averaged stats: model_time: 0.0933 (0.1042)  evaluator_time: 0.0057 (0.0061)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.01s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.806\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.800\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.925\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.180\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.865\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.865\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.856\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.950\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.866\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.857\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.930\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.922\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "Epoch: [8]  [0/3]  eta: 0:00:02  lr: 0.000050  loss: 0.0932 (0.0932)  loss_classifier: 0.0267 (0.0267)  loss_box_reg: 0.0101 (0.0101)  loss_mask: 0.0512 (0.0512)  loss_objectness: 0.0006 (0.0006)  loss_rpn_box_reg: 0.0045 (0.0045)  time: 0.8511  data: 0.2112  max mem: 3048\n",
      "Epoch: [8]  [2/3]  eta: 0:00:00  lr: 0.000050  loss: 0.0889 (0.0891)  loss_classifier: 0.0252 (0.0248)  loss_box_reg: 0.0101 (0.0101)  loss_mask: 0.0493 (0.0493)  loss_objectness: 0.0002 (0.0003)  loss_rpn_box_reg: 0.0045 (0.0046)  time: 0.6037  data: 0.0727  max mem: 3048\n",
      "Epoch: [8] Total time: 0:00:01 (0.6152 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:00  model_time: 0.0899 (0.0899)  evaluator_time: 0.0058 (0.0058)  time: 0.1892  data: 0.0915  max mem: 3048\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.0915 (0.0912)  evaluator_time: 0.0049 (0.0051)  time: 0.1175  data: 0.0197  max mem: 3048\n",
      "Test: Total time: 0:00:00 (0.1223 s / it)\n",
      "Averaged stats: model_time: 0.0915 (0.0912)  evaluator_time: 0.0049 (0.0051)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.801\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.875\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.875\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.861\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.875\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.866\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.935\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.935\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.928\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9]  [0/3]  eta: 0:00:01  lr: 0.000005  loss: 0.0830 (0.0830)  loss_classifier: 0.0153 (0.0153)  loss_box_reg: 0.0086 (0.0086)  loss_mask: 0.0517 (0.0517)  loss_objectness: 0.0008 (0.0008)  loss_rpn_box_reg: 0.0067 (0.0067)  time: 0.6639  data: 0.1657  max mem: 3048\n",
      "Epoch: [9]  [2/3]  eta: 0:00:00  lr: 0.000005  loss: 0.0830 (0.0859)  loss_classifier: 0.0182 (0.0211)  loss_box_reg: 0.0087 (0.0098)  loss_mask: 0.0517 (0.0499)  loss_objectness: 0.0002 (0.0004)  loss_rpn_box_reg: 0.0039 (0.0048)  time: 0.5101  data: 0.0572  max mem: 3048\n",
      "Epoch: [9] Total time: 0:00:01 (0.5180 s / it)\n",
      "creating index...\n",
      "index created!\n",
      "Test:  [0/5]  eta: 0:00:01  model_time: 0.1044 (0.1044)  evaluator_time: 0.0063 (0.0063)  time: 0.2025  data: 0.0887  max mem: 3048\n",
      "Test:  [4/5]  eta: 0:00:00  model_time: 0.1014 (0.1025)  evaluator_time: 0.0051 (0.0054)  time: 0.1300  data: 0.0198  max mem: 3048\n",
      "Test: Total time: 0:00:00 (0.1350 s / it)\n",
      "Averaged stats: model_time: 0.1014 (0.1025)  evaluator_time: 0.0051 (0.0054)\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.00s).\n",
      "IoU metric: bbox\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.810\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.801\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.185\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.875\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.875\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.861\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "IoU metric: segm\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.875\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.894\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.866\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.200\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.935\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.935\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.928\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 1.000\n",
      "That's it!\n"
     ]
    }
   ],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "\n",
    "# def main():\n",
    "# 학습을 GPU로 진행하되 GPU가 가용하지 않으면 CPU로 합니다\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# 우리 데이터셋은 두 개의 클래스만 가집니다 - 배경과 사람\n",
    "num_classes = 2\n",
    "# 데이터셋과 정의된 변환들을 사용합니다\n",
    "dataset = PennFudanDataset( get_transform(train=True))\n",
    "dataset_test = PennFudanDataset(get_transform(train=False))\n",
    "\n",
    "# dataset = PennFudanDataset()\n",
    "# dataset_test = PennFudanDataset()\n",
    "\n",
    "\n",
    "# 데이터셋을 학습용과 테스트용으로 나눕니다(역자주: 여기서는 전체의 50개를 테스트에, 나머지를 학습에 사용합니다)\n",
    "indices = torch.randperm(len(dataset)).tolist()\n",
    "dataset = torch.utils.data.Subset(dataset, indices[:4])\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, indices[:4])\n",
    "\n",
    "# 데이터 로더를 학습용과 검증용으로 정의합니다\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# 도움 함수를 이용해 모델을 가져옵니다\n",
    "model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "# 모델을 GPU나 CPU로 옮깁니다\n",
    "model.to(device)\n",
    "\n",
    "# 옵티마이저(Optimizer)를 만듭니다\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# 학습률 스케쥴러를 만듭니다\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=3,\n",
    "                                               gamma=0.1)\n",
    "\n",
    "# 10 에포크만큼 학습해봅시다\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 1 에포크동안 학습하고, 10회 마다 출력합니다\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # 학습률을 업데이트 합니다\n",
    "    lr_scheduler.step()\n",
    "    # 테스트 데이터셋에서 평가를 합니다\n",
    "    evaluate(model, data_loader_test, device=device)\n",
    "\n",
    "print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model.roi_heads.maskrcnn_loss??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[1]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=512x512 at 0x7F74B0892DA0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction[0]['masks'][0,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred=prediction[0]['masks'][0,0] +prediction[0]['masks'][1,0]+prediction[0]['masks'][2,0]+prediction[0]['masks'][3,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pred>=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=512x512 at 0x7F74B08921D0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(pred.mul(255).byte().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pick one image from the test set\n",
    "img, _ = dataset_test[1]\n",
    "# put the model in evaluation mode\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "pred=prediction[0]['masks'][0,0] +prediction[0]['masks'][1,0]+prediction[0]['masks'][2,0]+prediction[0]['masks'][3,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=512x512 at 0x7F74AF3416A0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(pred.mul(255).byte().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(pred.cpu().numpy())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(prediction[0]['masks'][2,0].cpu().numpy()>=0.5) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[0]['masks'][3,0].cpu().mul(255).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0000000e+00, 4.2097554e-08, 5.2477272e-08, ..., 2.5499997e+02,\n",
       "       2.5499998e+02, 2.5500000e+02], dtype=float32)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(prediction[0]['masks'].cpu().mul(255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[1.0496e+02, 1.4565e+02, 1.8704e+02, 2.0969e+02],\n",
       "          [2.3905e+02, 3.2184e+02, 3.0710e+02, 3.6915e+02],\n",
       "          [4.4019e+02, 3.8224e+02, 5.1142e+02, 4.5294e+02],\n",
       "          [0.0000e+00, 9.4191e+00, 7.4523e+01, 1.4140e+02],\n",
       "          [1.3011e+02, 1.4044e+02, 1.7737e+02, 2.0282e+02],\n",
       "          [1.3233e+02, 1.2441e+02, 2.0542e+02, 2.0711e+02],\n",
       "          [6.9282e+01, 1.3317e+02, 2.0010e+02, 2.0017e+02],\n",
       "          [2.1670e+02, 3.0220e+02, 3.0349e+02, 3.7573e+02],\n",
       "          [3.4310e+01, 4.0928e+00, 8.2468e+01, 1.3222e+02],\n",
       "          [1.1373e+02, 1.3071e+02, 1.9331e+02, 2.4436e+02],\n",
       "          [2.4206e+02, 3.0844e+02, 3.1016e+02, 4.0093e+02],\n",
       "          [4.6734e+02, 3.8908e+02, 5.0653e+02, 4.5605e+02],\n",
       "          [8.9400e+01, 1.0557e+02, 2.1360e+02, 2.1904e+02],\n",
       "          [1.1266e+02, 1.2907e+02, 1.6230e+02, 2.0009e+02],\n",
       "          [4.3317e+02, 3.8476e+02, 4.9618e+02, 4.9494e+02],\n",
       "          [2.4515e+02, 2.9480e+02, 3.2876e+02, 3.7265e+02],\n",
       "          [1.5109e+02, 1.4538e+02, 1.8673e+02, 2.0069e+02],\n",
       "          [4.3424e+02, 4.0301e+02, 5.0397e+02, 4.4425e+02],\n",
       "          [2.6397e+02, 3.2225e+02, 3.0291e+02, 3.5938e+02],\n",
       "          [2.5596e+02, 3.2191e+02, 2.9534e+02, 4.0423e+02],\n",
       "          [2.2911e+02, 3.1763e+02, 3.4336e+02, 3.8458e+02],\n",
       "          [2.7573e+02, 3.1796e+02, 3.0694e+02, 3.7163e+02],\n",
       "          [1.8205e+01, 0.0000e+00, 1.1077e+02, 1.6058e+02],\n",
       "          [4.4719e+02, 3.8988e+02, 4.8619e+02, 4.4834e+02],\n",
       "          [4.0798e+02, 3.2829e+02, 5.1200e+02, 4.8339e+02],\n",
       "          [2.4206e+02, 3.3230e+02, 2.8875e+02, 3.7524e+02],\n",
       "          [4.4848e+01, 2.3767e+01, 7.5796e+01, 1.1137e+02],\n",
       "          [4.6916e+02, 3.8087e+02, 5.1200e+02, 4.9887e+02],\n",
       "          [9.7984e+01, 1.3694e+02, 1.5763e+02, 2.2501e+02],\n",
       "          [0.0000e+00, 8.2078e+00, 3.7265e+01, 1.3437e+02],\n",
       "          [4.8401e+02, 3.9144e+02, 5.1117e+02, 4.5491e+02],\n",
       "          [1.3168e+02, 1.4729e+02, 2.0099e+02, 1.9012e+02],\n",
       "          [1.0358e+02, 9.4992e+00, 1.9789e+02, 2.3676e+02],\n",
       "          [2.7139e+02, 3.3022e+02, 2.8711e+02, 3.7230e+02],\n",
       "          [1.1981e+02, 1.4979e+02, 1.6312e+02, 2.2956e+02],\n",
       "          [2.1808e+02, 2.3237e+02, 3.2194e+02, 4.2713e+02],\n",
       "          [5.6910e+01, 3.9684e+01, 8.0351e+01, 1.1525e+02],\n",
       "          [2.5534e+02, 3.3063e+02, 2.7265e+02, 3.6862e+02],\n",
       "          [2.5154e+02, 3.2313e+02, 3.0421e+02, 4.4512e+02],\n",
       "          [2.8774e+02, 3.3140e+02, 3.0100e+02, 3.5946e+02],\n",
       "          [4.6693e+02, 3.3496e+02, 5.1200e+02, 4.5470e+02],\n",
       "          [2.3466e+02, 3.0837e+02, 3.8575e+02, 3.6722e+02],\n",
       "          [3.0591e+02, 3.7364e+02, 5.1200e+02, 4.6624e+02],\n",
       "          [2.4300e+00, 2.2025e-01, 2.0389e+02, 2.1137e+02],\n",
       "          [4.5566e+02, 3.9850e+02, 4.9420e+02, 4.7279e+02],\n",
       "          [2.7766e+02, 3.3276e+02, 2.9156e+02, 3.6841e+02],\n",
       "          [1.0880e+02, 1.4639e+02, 1.4470e+02, 2.0057e+02],\n",
       "          [4.4554e+02, 2.4745e+02, 5.1200e+02, 4.7667e+02],\n",
       "          [2.6086e+02, 3.3124e+02, 2.7786e+02, 3.7293e+02],\n",
       "          [2.9268e+02, 3.3368e+02, 3.0494e+02, 3.5895e+02],\n",
       "          [4.3752e+02, 3.9242e+02, 4.6906e+02, 4.4674e+02],\n",
       "          [0.0000e+00, 1.4376e+02, 1.7817e+02, 2.5935e+02],\n",
       "          [4.3818e+02, 4.0845e+02, 5.1200e+02, 4.7347e+02],\n",
       "          [2.8773e+02, 3.3831e+02, 3.0184e+02, 3.7189e+02],\n",
       "          [2.8104e+02, 3.3687e+02, 3.0594e+02, 3.6126e+02],\n",
       "          [1.9302e+00, 2.3971e+00, 1.3636e+02, 1.0149e+02],\n",
       "          [1.0725e+02, 8.8226e+01, 2.0448e+02, 3.2319e+02],\n",
       "          [2.6924e+01, 4.4518e+01, 5.7889e+01, 1.2083e+02],\n",
       "          [0.0000e+00, 6.8451e+01, 9.8652e+01, 1.3826e+02],\n",
       "          [2.6520e+02, 3.3576e+02, 2.8313e+02, 3.8076e+02],\n",
       "          [2.4132e+01, 1.2739e+02, 2.8715e+02, 2.0488e+02],\n",
       "          [2.6408e+02, 3.3967e+02, 2.9414e+02, 3.5271e+02],\n",
       "          [2.9274e+02, 3.4097e+02, 3.0578e+02, 3.7638e+02]], device='cuda:0'),\n",
       "  'labels': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0'),\n",
       "  'scores': tensor([0.8726, 0.8554, 0.8272, 0.8229, 0.7587, 0.6009, 0.5908, 0.5888, 0.5792,\n",
       "          0.5588, 0.5513, 0.5275, 0.4758, 0.4705, 0.4403, 0.4307, 0.4264, 0.4236,\n",
       "          0.4055, 0.3916, 0.3905, 0.3869, 0.3751, 0.3507, 0.3247, 0.2784, 0.2711,\n",
       "          0.2669, 0.2522, 0.2346, 0.2334, 0.2330, 0.2154, 0.1879, 0.1786, 0.1710,\n",
       "          0.1678, 0.1666, 0.1605, 0.1577, 0.1433, 0.1398, 0.1395, 0.1386, 0.1369,\n",
       "          0.1289, 0.1214, 0.1106, 0.1088, 0.1053, 0.1042, 0.0899, 0.0872, 0.0807,\n",
       "          0.0794, 0.0709, 0.0641, 0.0594, 0.0583, 0.0566, 0.0555, 0.0535, 0.0535],\n",
       "         device='cuda:0'),\n",
       "  'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          ...,\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "  \n",
       "  \n",
       "          [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]], device='cuda:0')}]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
